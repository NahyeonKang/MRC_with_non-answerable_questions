{"cells":[{"cell_type":"markdown","metadata":{"id":"LvbiXYSVQAxn"},"source":["# Install"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3858,"status":"ok","timestamp":1668704938225,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"},"user_tz":-540},"id":"R-PJ_Loo_TiN","outputId":"6fa0f6ab-5ce3-4f44-ef0d-1ac8ac8276a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting attrdict\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n","Installing collected packages: attrdict\n","Successfully installed attrdict-2.0.1\n"]}],"source":["! pip install attrdict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8770,"status":"ok","timestamp":1668704946991,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"},"user_tz":-540},"id":"bdgCkSl1_WXF","outputId":"e5bedc59-ed07-47c3-ed89-b7d62f7f25ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 4.8 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 87.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 79.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"]}],"source":["! pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6706,"status":"ok","timestamp":1668704953692,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"},"user_tz":-540},"id":"PKWjxp8j_ip5","outputId":"2b13410d-d8c7-4cd1-aaa6-c096bf63b021"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16182 sha256=a17e85ef5c952776af83268033414b833c8bad32c3e9bb6ebc082736d7ee46a1\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}],"source":["! pip install seqeval"]},{"cell_type":"markdown","metadata":{"id":"Ty4IOYDUxJOA"},"source":["# Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3441,"status":"ok","timestamp":1668720291399,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"},"user_tz":-540},"id":"t3pQd81UK-04"},"outputs":[],"source":["from collections import defaultdict\n","import argparse\n","import json\n","import glob\n","import os\n","import random\n","import timeit\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from fastprogress.fastprogress import master_bar, progress_bar\n","from attrdict import AttrDict\n","from transformers import (\n","    AdamW,\n","    get_linear_schedule_with_warmup,\n","    squad_convert_examples_to_features\n",")\n","from transformers.data.metrics.squad_metrics import (\n","    compute_predictions_logits,\n","    squad_evaluate,\n",")\n","from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n","\n","def to_list(tensor):\n","  return tensor.detach().cpu().tolist()"]},{"cell_type":"markdown","metadata":{"id":"ahOvK-POjc30"},"source":["# utils"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1668720292640,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"},"user_tz":-540},"id":"OsSBEnicjff5"},"outputs":[],"source":["import random\n","from numpy.lib.function_base import average\n","\n","import torch\n","import numpy as np\n","\n","from scipy.stats import pearsonr, spearmanr\n","from seqeval import metrics as seqeval_metrics\n","from sklearn import metrics as sklearn_metrics\n","from transformers import (\n","    AlbertConfig,\n","    DistilBertConfig,\n","    ElectraConfig,\n","    BertTokenizer,\n","    AlbertTokenizer,\n","    ElectraTokenizer,\n","    AlbertForSequenceClassification,\n","    DistilBertForSequenceClassification,\n","    ElectraForSequenceClassification,\n","    AlbertForTokenClassification,\n","    DistilBertForTokenClassification,\n","    ElectraForTokenClassification,\n","    AlbertForQuestionAnswering,\n","    DistilBertForQuestionAnswering,\n","    ElectraForQuestionAnswering,\n",")\n","\n","CONFIG_CLASSES = {\n","    \"albert\": AlbertConfig,\n","    \"distilkobert\": DistilBertConfig,\n","    \"koelectra-base\": ElectraConfig,\n","    \"koelectra-small\": ElectraConfig,\n","    \"koelectra-base-v2\": ElectraConfig,\n","    \"koelectra-base-v3\": ElectraConfig,\n","    \"koelectra-small-v2\": ElectraConfig,\n","    \"koelectra-small-v3\": ElectraConfig,\n","}\n","\n","TOKENIZER_CLASSES = {\n","    \"albert\": BertTokenizer,\n","    \"distilkobert\": BertTokenizer,\n","    \"koelectra-base\": ElectraTokenizer,\n","    \"koelectra-small\": ElectraTokenizer,\n","    \"koelectra-base-v2\": ElectraTokenizer,\n","    \"koelectra-base-v3\": ElectraTokenizer,\n","    \"koelectra-small-v2\": ElectraTokenizer,\n","    \"koelectra-small-v3\": ElectraTokenizer,\n","}\n","\n","MODEL_FOR_SEQUENCE_CLASSIFICATION = {\n","    \"albert\": AlbertForSequenceClassification,\n","    \"distilkobert\": DistilBertForSequenceClassification,\n","    \"koelectra-base\": ElectraForSequenceClassification,\n","    \"koelectra-small\": ElectraForSequenceClassification,\n","    \"koelectra-base-v2\": ElectraForSequenceClassification,\n","    \"koelectra-base-v3\": ElectraForSequenceClassification,\n","    \"koelectra-small-v2\": ElectraForSequenceClassification,\n","    \"koelectra-small-v3\": ElectraForSequenceClassification,\n","}\n","\n","MODEL_FOR_TOKEN_CLASSIFICATION = {\n","    \"albert\": AlbertForTokenClassification,\n","    \"distilkobert\": DistilBertForTokenClassification,\n","    \"koelectra-base\": ElectraForTokenClassification,\n","    \"koelectra-small\": ElectraForTokenClassification,\n","    \"koelectra-base-v2\": ElectraForTokenClassification,\n","    \"koelectra-base-v3\": ElectraForTokenClassification,\n","    \"koelectra-small-v2\": ElectraForTokenClassification,\n","    \"koelectra-small-v3\": ElectraForTokenClassification,\n","    \"koelectra-small-v3-51000\": ElectraForTokenClassification,\n","}\n","\n","MODEL_FOR_QUESTION_ANSWERING = {\n","    \"albert\": AlbertForQuestionAnswering,\n","    \"distilkobert\": DistilBertForQuestionAnswering,\n","    \"koelectra-base\": ElectraForQuestionAnswering,\n","    \"koelectra-small\": ElectraForQuestionAnswering,\n","    \"koelectra-base-v2\": ElectraForQuestionAnswering,\n","    \"koelectra-base-v3\": ElectraForQuestionAnswering,\n","    \"koelectra-small-v2\": ElectraForQuestionAnswering,\n","    \"koelectra-small-v3\": ElectraForQuestionAnswering,\n","}\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if not args.no_cuda and torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(args.seed)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1668720293065,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"},"user_tz":-540},"id":"WYXzLdhyO_0G"},"outputs":[],"source":["def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n","    # Load data features from cache or dataset file\n","    input_dir = args.data_dir if args.data_dir else \".\"\n","    cached_features_file = os.path.join(\n","        input_dir,\n","        \"cached_{}_{}_{}\".format(\n","            \"dev\" if evaluate else \"train\",\n","            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","            str(args.max_seq_length),\n","        ),\n","    )\n","\n","    # Init features and dataset from cache if it exists\n","    if os.path.exists(cached_features_file):\n","        print(\"Loading features from cached file %s\"% cached_features_file)\n","        features_and_dataset = torch.load(cached_features_file)\n","        features, dataset, examples = (\n","            features_and_dataset[\"features\"],\n","            features_and_dataset[\"dataset\"],\n","            features_and_dataset[\"examples\"],\n","        )\n","    else:\n","        print(\"Creating features from dataset file at %s\"%input_dir)\n","\n","        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n","            try:\n","                import tensorflow_datasets as tfds\n","            except ImportError:\n","                raise ImportError(\"If data_dir is not specified, tensorflow_datasets needs to be installed.\")\n","\n","            if args.version_2_with_negative:\n","                print(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n","\n","            tfds_examples = tfds.load(\"squad\")\n","            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n","        else:\n","            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n","            if evaluate:\n","                examples = processor.get_dev_examples(os.path.join(args.data_dir, args.task),\n","                                                      filename=args.predict_file)\n","            else:\n","                examples = processor.get_train_examples(os.path.join(args.data_dir, args.task),\n","                                                        filename=args.train_file)\n","\n","        features, dataset = squad_convert_examples_to_features(\n","            examples=examples,\n","            tokenizer=tokenizer,\n","            max_seq_length=args.max_seq_length,\n","            doc_stride=args.doc_stride,\n","            max_query_length=args.max_query_length,\n","            is_training=not evaluate,\n","            return_dataset=\"pt\",\n","            threads=args.threads,\n","        )\n","\n","        print(\"Saving features into cached file %s\"% cached_features_file)\n","        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n","\n","    if output_examples:\n","        return dataset, examples, features\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"_Ev7GgGhWPYb"},"source":["# Sketchy Reading Module"]},{"cell_type":"markdown","metadata":{"id":"dH6z0ARFwctZ"},"source":["## Train"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"TmsxBXZKLS2a","executionInfo":{"status":"ok","timestamp":1668720294478,"user_tz":-540,"elapsed":3,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[],"source":["def train(args, train_dataset, model, tokenizer):\n","  train_sampler = RandomSampler(train_dataset)\n","  train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size = args.train_batch_size)\n","\n","  if args.max_steps > 0:\n","    t_total = args.max_steps\n","    args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","  else:\n","    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","  no_decay = [\"bias\", \"LayerNorm.weight\"]\n","  optimizer_grouped_parameters = [\n","      {\n","          \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","          \"weight_decay\": args.weight_decay,\n","      },\n","      {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","  ]\n","  optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","  scheduler = get_linear_schedule_with_warmup(\n","      optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n","  )\n","\n","  # Check if saved optimizer or scheduler states exist\n","  if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n","          os.path.join(args.model_name_or_path, \"scheduler.pt\")\n","  ):\n","      # Load in optimizer and scheduler states\n","      optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","      scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","  # Train\n","  print(\"***** Running training *****\")\n","  print(\"  Num examples = %d\" % len(train_dataset))\n","  print(\"  Num Epochs = %d\" % args.num_train_epochs)\n","  print(\"  Train batch size per GPU = %d\"% args.train_batch_size)\n","  print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\"% (args.train_batch_size * args.gradient_accumulation_steps))\n","  print(\"  Gradient Accumulation steps = %d\"% args.gradient_accumulation_steps)\n","  print(\"  Total optimization steps = %d\"% t_total)\n","\n","  global_step = 1\n","  steps_trained_in_current_epoch = 0\n","  # Check if continuing training from a checkpoint\n","  if os.path.exists(args.model_name_or_path):\n","      try:\n","          # set global_step to global_step of last saved checkpoint from model path\n","          checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","          global_step = int(checkpoint_suffix)\n","          epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","          steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","          print(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","          print(\"  Continuing training from epoch %d\"% epochs_trained)\n","          print(\"  Continuing training from global step %d\"% global_step)\n","          print(\"  Will skip the first %d steps in the first epoch\"% steps_trained_in_current_epoch)\n","      except ValueError:\n","          print(\"  Starting fine-tuning.\")\n","\n","  tr_loss, logging_loss = 0.0, 0.0\n","  model.zero_grad()\n","  mb = master_bar(range(int(args.num_train_epochs)))\n","  # Added here for reproductibility\n","  set_seed(args)\n","\n","  for epoch in mb:\n","      epoch_iterator = progress_bar(train_dataloader, parent=mb)\n","      for step, batch in enumerate(epoch_iterator):\n","          # Skip past any already trained steps if resuming training\n","          if steps_trained_in_current_epoch > 0:\n","              steps_trained_in_current_epoch -= 1\n","              continue\n","\n","          model.train()\n","          batch = tuple(t.to(args.device, dtype=torch.long) for t in batch)\n","\n","          inputs = {\n","              \"input_ids\": batch[0],\n","              \"attention_mask\": batch[1],\n","              \"token_type_ids\": batch[2],\n","              \"labels\": batch[7],\n","          }\n","\n","\n","          outputs = model(**inputs)\n","          # model outputs are always tuple in transformers (see doc)\n","          loss = outputs[0]\n","\n","          if args.gradient_accumulation_steps > 1:\n","              loss = loss / args.gradient_accumulation_steps\n","\n","          loss.backward()\n","\n","          tr_loss += loss.item()\n","          if (step + 1) % args.gradient_accumulation_steps == 0:\n","              torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","              optimizer.step()\n","              scheduler.step()  # Update learning rate schedule\n","              model.zero_grad()\n","              global_step += 1\n","              \n","\n","              # Save model checkpoint\n","              if args.save_steps > 0 and global_step % args.save_steps == 0:\n","                  output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n","                  if not os.path.exists(output_dir):\n","                      os.makedirs(output_dir)\n","                  # Take care of distributed/parallel training\n","                  model_to_save = model.module if hasattr(model, \"module\") else model\n","                  model_to_save.save_pretrained(output_dir)\n","                  tokenizer.save_pretrained(output_dir)\n","\n","                  torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                  print(\"Saving model checkpoint to %s\"% output_dir)\n","\n","                  if args.save_optimizer:\n","                      torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                      torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                      print(\"Saving optimizer and scheduler states to %s\"% output_dir)\n","\n","          if args.max_steps > 0 and global_step > args.max_steps:\n","              break\n","\n","      mb.write(\"Epoch {} done\".format(epoch+1))\n","\n","      if args.max_steps > 0 and global_step > args.max_steps:\n","          break\n","\n","  return global_step, tr_loss / global_step"]},{"cell_type":"markdown","metadata":{"id":"BWUXpEH4wfnQ"},"source":["## Evaluate"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"k_2I5-T6OIT1","executionInfo":{"status":"ok","timestamp":1668720296860,"user_tz":-540,"elapsed":289,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[],"source":["from sklearn.metrics import f1_score\n","def evaluate(args, model, tokenizer, global_step=None):\n","    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(dataset)\n","    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","\n","    # Eval!\n","    print(\"***** Running evaluation {} *****\".format(global_step))\n","    print(\"  Num examples = %d\"% len(examples))\n","    print(\"  Num features = %d\"% len(features))\n","    print(\"  Batch size = %d\"% args.eval_batch_size)\n","\n","    start_time = timeit.default_timer()\n","\n","    results = {}\n","    num_id = 0\n","    preds = None\n","    out_label_ids = None\n","    key_map = {}\n","    cnt_map = {}\n","\n","    id_map = [feature.qas_id for feature in features]\n","    # if args.write_qas_id:\n","    #     id_map = [feature.qas_id for feature in features]\n","    # else:\n","    #     id_map = [feature.example_index for feature in features]\n","\n","    out_label_ids = [feature.is_impossible for feature in features]\n","\n","    example_idx_to_label = defaultdict(int)\n","\n","    for idx, example in enumerate(examples):\n","        example_idx_to_label[idx] = example.is_impossible\n","\n","    for batch in progress_bar(eval_dataloader):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids': batch[0],\n","                    'attention_mask': batch[1],\n","                    'token_type_ids': batch[2]\n","                    }\n","\n","            logits = model(**inputs)\n","            logits = logits[0].detach().cpu().numpy()\n","\n","        # batch내의 모든 예측값에 대해서\n","        for logit in logits:\n","            qas_id = id_map[num_id]\n","            \n","            if qas_id in key_map:\n","                logit_list = key_map[qas_id]\n","                logit_list[0] += logit[0]\n","                logit_list[1] += logit[1]\n","                cnt_map[qas_id] += 1\n","\n","            else:\n","                cnt_map[qas_id] = 1\n","                key_map[qas_id] = [logit[0], logit[1]]\n","            \n","            num_id += 1\n","    \n","        if preds is None:\n","            preds = logits\n","\n","        else:\n","            preds = np.append(preds, logits, axis=0)\n","    \n","    preds = np.argmax(preds, axis=1).tolist()\n","\n","\n","\n","    evalTime = timeit.default_timer() - start_time\n","    print(\"  Evaluation done in total %f secs (%f sec per example)\"% (evalTime, evalTime / len(dataset)))\n","\n","    result = {\"f1\": f1_score(y_true=out_label_ids, y_pred=preds)}\n","    results.update(result)\n","\n","    final_map = {}\n","    for idx, key in enumerate(key_map):\n","        key_list = key_map[key]\n","        key_list[0] = key_list[0] / cnt_map[key]\n","        key_list[1] = key_list[1] / cnt_map[key]\n","        final_map[key] = key_list[1] - key_list[0]\n","\n","    with open(os.path.join(args.output_dir, \"cls_score.json\"), \"w\") as writer:\n","        writer.write(json.dumps(final_map, indent=4, ensure_ascii=False) + \"\\n\")\n","\n","    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","    with open(output_eval_file, \"a\") as writer:\n","        print(\"***** Eval results *****\")\n","        writer.write(\"***** Eval results *****\")\n","        for key in sorted(result.keys()):\n","            print(\"  %s = %s\"% (key, str(result[key])))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"dJOKs50_wh3E"},"source":["## RUN"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"G7eZJXAqRErD","executionInfo":{"status":"ok","timestamp":1668720299986,"user_tz":-540,"elapsed":291,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[],"source":["def main(cli_args):\n","    # Read from config file and make args\n","    with open(os.path.join(cli_args.config_dir, cli_args.task, cli_args.config_file)) as f:\n","        args = AttrDict(json.load(f))\n","    print(\"Training/evaluation parameters {}\".format(args))\n","\n","    args.output_dir = os.path.join(args.ckpt_dir, args.output_dir)\n","\n","    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n","        print(\n","            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n","            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n","            \"stride or increase the maximum length to ensure the features are correctly built.\"\n","        )\n","    set_seed(args)\n","\n","    # Load pretrained model and tokenizer\n","    config = CONFIG_CLASSES[args.model_type].from_pretrained(\n","        args.model_name_or_path,\n","    )\n","    tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n","        args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","    )\n","    model = MODEL_FOR_SEQUENCE_CLASSIFICATION[args.model_type].from_pretrained(\n","        args.model_name_or_path,\n","        config=config,\n","    )\n","    # GPU or CPU\n","    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","    model.to(args.device)\n","\n","    print(\"Training/evaluation parameters %s\"% args)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","        print(\" global_step = %s, average loss = %s\"% (global_step, tr_loss))\n","\n","    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n","    results = {}\n","\n","    if args.do_eval:\n","        checkpoints = list(\n","            os.path.dirname(c)\n","            for c in sorted(glob.glob(\"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule\" + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n","        )\n","\n","        print(\"Evaluate the following checkpoints: %s\"% checkpoints)\n","\n","        for checkpoint in checkpoints:\n","            # Reload the model\n","            global_step = checkpoint.split(\"-\")[-1]\n","            model = MODEL_FOR_SEQUENCE_CLASSIFICATION[\"albert\"].from_pretrained(checkpoint)\n","            model.to(args.device)\n","            args.output_dir = os.path.join(args.output_dir, checkpoint)\n","            result = evaluate(args, model, tokenizer, global_step=global_step)\n","            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n","            results.update(result)\n","\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results_sketchy.txt\")\n","        with open(output_eval_file, \"w\") as f_w:\n","            for key in sorted(results.keys()):\n","                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaJ92JXTIawU"},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SPfkqo94SbmH","outputId":"9f6c1c25-134f-4a36-f342-c67ae5953eb0","executionInfo":{"status":"ok","timestamp":1668727685115,"user_tz":-540,"elapsed":7222916,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'sop_classifier.classifier.weight', 'predictions.dense.weight', 'sop_classifier.classifier.bias']\n","- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Training/evaluation parameters AttrDict({'task': 'news', 'data_dir': '/content/drive/MyDrive/Colab Notebooks/KB/data', 'ckpt_dir': '/content/drive/MyDrive/Colab Notebooks/KB/ckpt', 'train_file': 'TL.json', 'predict_file': 'VL.json', 'threads': 20, 'version_2_with_negative': True, 'null_score_diff_threshold': 0.0, 'max_seq_length': 512, 'doc_stride': 400, 'max_query_length': 100, 'max_answer_length': 100, 'n_best_size': 20, 'verbose_logging': True, 'overwrite_output_dir': True, 'evaluate_during_training': True, 'eval_all_checkpoints': True, 'select_checkpoint': 'checkpoint-51000', 'save_optimizer': False, 'do_lower_case': False, 'do_train': True, 'do_eval': True, 'n_gpu': 1, 'num_train_epochs': 3, 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'max_steps': -1, 'max_grad_norm': 1.0, 'no_cuda': False, 'model_type': 'albert', 'model_name_or_path': '/content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2', 'output_dir': 'SketchReadingModule', 'seed': 42, 'train_batch_size': 16, 'eval_batch_size': 128, 'logging_steps': 4000, 'save_steps': 4000, 'learning_rate': 5e-05, 'write_qas_id': True})\n","Training/evaluation parameters AttrDict({'task': 'news', 'data_dir': '/content/drive/MyDrive/Colab Notebooks/KB/data', 'ckpt_dir': '/content/drive/MyDrive/Colab Notebooks/KB/ckpt', 'train_file': 'TL.json', 'predict_file': 'VL.json', 'threads': 20, 'version_2_with_negative': True, 'null_score_diff_threshold': 0.0, 'max_seq_length': 512, 'doc_stride': 400, 'max_query_length': 100, 'max_answer_length': 100, 'n_best_size': 20, 'verbose_logging': True, 'overwrite_output_dir': True, 'evaluate_during_training': True, 'eval_all_checkpoints': True, 'select_checkpoint': 'checkpoint-51000', 'save_optimizer': False, 'do_lower_case': False, 'do_train': True, 'do_eval': True, 'n_gpu': 1, 'num_train_epochs': 3, 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'max_steps': -1, 'max_grad_norm': 1.0, 'no_cuda': False, 'model_type': 'albert', 'model_name_or_path': '/content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2', 'output_dir': '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule', 'seed': 42, 'train_batch_size': 16, 'eval_batch_size': 128, 'logging_steps': 4000, 'save_steps': 4000, 'learning_rate': 5e-05, 'write_qas_id': True, 'device': 'cuda'})\n","Creating features from dataset file at /content/drive/MyDrive/Colab Notebooks/KB/data\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 46486/46486 [00:16<00:00, 2763.71it/s]\n","convert squad examples to features: 100%|██████████| 92980/92980 [01:28<00:00, 1046.06it/s]\n","add example index and unique id: 100%|██████████| 92980/92980 [00:00<00:00, 893279.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saving features into cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_train_kb-albert-char-base-v2_512\n","***** Running training *****\n","  Num examples = 92980\n","  Num Epochs = 3\n","  Train batch size per GPU = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 17436\n","  Starting fine-tuning.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Epoch 1 done<p>Epoch 2 done<p>Epoch 3 done"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-4000\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-8000\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-12000\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-16000\n"," global_step = 17437, average loss = 0.2082437890372223\n","Evaluate the following checkpoints: ['/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-12000', '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-16000', '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-4000', '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-8000']\n","Creating features from dataset file at /content/drive/MyDrive/Colab Notebooks/KB/data\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13519/13519 [00:05<00:00, 2465.45it/s]\n","convert squad examples to features: 100%|██████████| 27038/27038 [00:26<00:00, 1007.47it/s]\n","add example index and unique id: 100%|██████████| 27038/27038 [00:00<00:00, 775391.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saving features into cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 12000 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:02&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 182.102888 secs (0.006735 sec per example)\n","***** Eval results *****\n","  f1 = 0.9310478271756698\n","Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 16000 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 180.606842 secs (0.006680 sec per example)\n","***** Eval results *****\n","  f1 = 0.9298181485881385\n","Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 4000 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 180.635217 secs (0.006681 sec per example)\n","***** Eval results *****\n","  f1 = 0.9118766830717802\n","Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 8000 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 180.593479 secs (0.006679 sec per example)\n","***** Eval results *****\n","  f1 = 0.9282514209294549\n"]}],"source":["import easydict \n","cli_args = easydict.EasyDict({\n","    \"task\": \"news\",\n","    \"config_dir\": \"/content/drive/MyDrive/Colab Notebooks/KB/config\",\n","    \"config_file\": \"SketchReadingModule_train_evaluate.json\"\n","\n","})\n","\n","main(cli_args)"]},{"cell_type":"markdown","metadata":{"id":"FRzWoZdZWTFN"},"source":["# Intensive Reading Module"]},{"cell_type":"markdown","metadata":{"id":"UlxU5eO4Kaqm"},"source":["## Train"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"DmU384u8WV7A","executionInfo":{"status":"ok","timestamp":1668727685116,"user_tz":-540,"elapsed":7,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[],"source":["def train(args, train_dataset, model, tokenizer):\n","    \"\"\" Train the model \"\"\"\n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n","            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    # Train!\n","    print(\"***** Running training *****\")\n","    print(\"  Num examples = %d\"% len(train_dataset))\n","    print(\"  Num Epochs = %d\"% args.num_train_epochs)\n","    print(\"  Train batch size per GPU = %d\"% args.train_batch_size)\n","    print(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\"% args.train_batch_size * args.gradient_accumulation_steps)\n","    print(\"  Gradient Accumulation steps = %d\"% args.gradient_accumulation_steps)\n","    print(\"  Total optimization steps = %d\"% t_total)\n","\n","    global_step = 1\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            print(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            print(\"  Continuing training from epoch %d\"% epochs_trained)\n","            print(\"  Continuing training from global step %d\"% global_step)\n","            print(\"  Will skip the first %d steps in the first epoch\"% steps_trained_in_current_epoch)\n","        except ValueError:\n","            print(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    mb = master_bar(range(int(args.num_train_epochs)))\n","    # Added here for reproductibility\n","    set_seed(args)\n","\n","    for epoch in mb:\n","        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n","        for step, batch in enumerate(epoch_iterator):\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            model.train()\n","            batch = tuple(t.to(args.device) for t in batch)\n","\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","                \"start_positions\": batch[3],\n","                \"end_positions\": batch[4],\n","            }\n","\n","            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n","                del inputs[\"token_type_ids\"]\n","\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                # Save model checkpoint\n","                if args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    # Take care of distributed/parallel training\n","                    model_to_save = model.module if hasattr(model, \"module\") else model\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    print(\"Saving model checkpoint to %s\"% output_dir)\n","\n","                    if args.save_optimizer:\n","                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                        print(\"Saving optimizer and scheduler states to %s\"% output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                break\n","\n","        mb.write(\"Epoch {} done\".format(epoch+1))\n","\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            break\n","\n","    return global_step, tr_loss / global_step"]},{"cell_type":"markdown","metadata":{"id":"rtz6JAdPKdPA"},"source":["## Evaluate"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"XwPp7D0CYJDz","executionInfo":{"status":"ok","timestamp":1668727685116,"user_tz":-540,"elapsed":6,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[],"source":["def evaluate(args, model, tokenizer, global_step=None):\n","    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(dataset)\n","    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n","\n","    # Eval!\n","    print(\"***** Running evaluation {} *****\".format(global_step))\n","    print(\"  Num examples = %d\"% len(examples))\n","    print(\"  Num features = %d\"% len(features))\n","    print(\"  Batch size = %d\"% args.eval_batch_size)\n","\n","    all_results = []\n","    start_time = timeit.default_timer()\n","\n","    for batch in progress_bar(eval_dataloader):\n","        model.eval()\n","        batch = tuple(t.to(args.device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","            }\n","\n","            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n","                del inputs[\"token_type_ids\"]\n","\n","            example_indices = batch[3]\n","\n","            outputs = model(**inputs)\n","\n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","\n","            output = [to_list(output[i]) for output in outputs.values()]\n","\n","            start_logits, end_logits = output\n","            result = SquadResult(unique_id, start_logits, end_logits)\n","\n","            all_results.append(result)\n","\n","    evalTime = timeit.default_timer() - start_time\n","    print(\"  Evaluation done in total %f secs (%f sec per example)\"% (evalTime, evalTime / len(dataset)))\n","\n","    # Compute predictions\n","    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(global_step))\n","    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(global_step))\n","\n","    if args.version_2_with_negative:\n","        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(global_step))\n","    else:\n","        output_null_log_odds_file = None\n","\n","    predictions = compute_predictions_logits(\n","        examples,\n","        features,\n","        all_results,\n","        args.n_best_size,\n","        args.max_answer_length,\n","        args.do_lower_case,\n","        output_prediction_file,\n","        output_nbest_file,\n","        output_null_log_odds_file,\n","        args.verbose_logging,\n","        args.version_2_with_negative,\n","        args.null_score_diff_threshold,\n","        tokenizer,\n","    )\n","\n","    # Compute the F1 and exact scores.\n","    results = squad_evaluate(examples, predictions)\n","    # Write the result\n","    # Write the evaluation result on file\n","    output_dir = os.path.join(args.output_dir, 'eval')\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    output_eval_file = os.path.join(output_dir, \"eval_result_{}_{}.txt\".format(list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","                                                                               global_step))\n","\n","    with open(output_eval_file, \"w\", encoding='utf-8') as f:\n","        json.dump(results, f, ensure_ascii=False)\n","\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"QoWO6Wy3KgBw"},"source":["## Run"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"lg5xHjATb4vR","executionInfo":{"status":"ok","timestamp":1668727685117,"user_tz":-540,"elapsed":7,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[],"source":["def main(cli_args):\n","    # Read from config file and make args\n","    with open(os.path.join(cli_args.config_dir, cli_args.task, cli_args.config_file)) as f:\n","        args = AttrDict(json.load(f))\n","    print(\"Training/evaluation parameters {}\".format(args))\n","\n","    args.output_dir = os.path.join(args.ckpt_dir, args.output_dir)\n","\n","    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n","        print(\n","            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n","            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n","            \"stride or increase the maximum length to ensure the features are correctly built.\"\n","        )\n","\n","    set_seed(args)\n","\n","  \n","    # Load pretrained model and tokenizer\n","    config = CONFIG_CLASSES[args.model_type].from_pretrained(\n","        args.model_name_or_path,\n","    )\n","    tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n","        args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","    )\n","    model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(\n","        args.model_name_or_path,\n","        config=config,\n","    )\n","    # GPU or CPU\n","    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","    model.to(args.device)\n","\n","    print(\"Training/evaluation parameters %s\"% args)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","        print(\" global_step = %s, average loss = %s\"% (global_step, tr_loss))\n","\n","    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n","    results = {}\n","    if args.do_eval:\n","        checkpoints = list(\n","            os.path.dirname(c)\n","            for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n","        )\n","        if not args.eval_all_checkpoints:\n","            checkpoints = [checkpoint for checkpoint in checkpoints if checkpoint.find(args.select_checkpoint) != -1]\n","\n","        print(\"Evaluate the following checkpoints: %s\"% checkpoints)\n","\n","        for checkpoint in checkpoints:\n","            # Reload the model\n","            global_step = checkpoint.split(\"-\")[-1]\n","            model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(checkpoint)\n","            model.to(args.device)\n","            args.output_dir = os.path.join(args.output_dir, checkpoint)\n","            result = evaluate(args, model, tokenizer, global_step=global_step)\n","            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n","            results.update(result)\n","\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as f_w:\n","            for key in sorted(results.keys()):\n","                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myiW4Oqftg5H"},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":586},"id":"Fk30iLq1cEK4","outputId":"f7704670-68b5-4b96-d7e6-111394768503","executionInfo":{"status":"ok","timestamp":1668736665948,"user_tz":-540,"elapsed":910071,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'sop_classifier.classifier.weight', 'predictions.dense.weight', 'sop_classifier.classifier.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Training/evaluation parameters AttrDict({'task': 'news', 'data_dir': '/content/drive/MyDrive/Colab Notebooks/KB/data', 'ckpt_dir': '/content/drive/MyDrive/Colab Notebooks/KB/ckpt', 'train_file': 'TL.json', 'predict_file': 'VL.json', 'threads': 20, 'version_2_with_negative': True, 'null_score_diff_threshold': 0.2, 'max_seq_length': 512, 'doc_stride': 400, 'max_query_length': 100, 'max_answer_length': 100, 'n_best_size': 20, 'verbose_logging': True, 'overwrite_output_dir': True, 'evaluate_during_training': True, 'eval_all_checkpoints': True, 'select_checkpoint': 'checkpoint-51000', 'save_optimizer': False, 'do_lower_case': False, 'do_train': False, 'do_eval': True, 'n_gpu': 1, 'num_train_epochs': 3, 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'max_steps': -1, 'max_grad_norm': 1.0, 'no_cuda': False, 'model_type': 'albert', 'model_name_or_path': '/content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2', 'output_dir': 'IntensiveReadingModule', 'seed': 42, 'train_batch_size': 16, 'eval_batch_size': 128, 'logging_steps': 5812, 'save_steps': 5812, 'learning_rate': 5e-05})\n","Training/evaluation parameters AttrDict({'task': 'news', 'data_dir': '/content/drive/MyDrive/Colab Notebooks/KB/data', 'ckpt_dir': '/content/drive/MyDrive/Colab Notebooks/KB/ckpt', 'train_file': 'TL.json', 'predict_file': 'VL.json', 'threads': 20, 'version_2_with_negative': True, 'null_score_diff_threshold': 0.2, 'max_seq_length': 512, 'doc_stride': 400, 'max_query_length': 100, 'max_answer_length': 100, 'n_best_size': 20, 'verbose_logging': True, 'overwrite_output_dir': True, 'evaluate_during_training': True, 'eval_all_checkpoints': True, 'select_checkpoint': 'checkpoint-51000', 'save_optimizer': False, 'do_lower_case': False, 'do_train': False, 'do_eval': True, 'n_gpu': 1, 'num_train_epochs': 3, 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': 1e-08, 'warmup_proportion': 0, 'max_steps': -1, 'max_grad_norm': 1.0, 'no_cuda': False, 'model_type': 'albert', 'model_name_or_path': '/content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2', 'output_dir': '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule', 'seed': 42, 'train_batch_size': 16, 'eval_batch_size': 128, 'logging_steps': 5812, 'save_steps': 5812, 'learning_rate': 5e-05, 'device': 'cuda'})\n","Evaluate the following checkpoints: ['/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624', '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-17436', '/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-5812']\n","Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 11624 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:05&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 185.482622 secs (0.006860 sec per example)\n","Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 17436 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:05&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 185.249944 secs (0.006851 sec per example)\n","Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n","***** Running evaluation 5812 *****\n","  Num examples = 27038\n","  Num features = 27038\n","  Batch size = 128\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='212' class='' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [212/212 03:05&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Evaluation done in total 185.305591 secs (0.006854 sec per example)\n"]}],"source":["import easydict \n","cli_args = easydict.EasyDict({\n","    \"task\": \"news\",\n","    \"config_dir\": \"/content/drive/MyDrive/Colab Notebooks/KB/config\",\n","    \"config_file\": \"IntensiveReadingModule_train_evaluate.json\"\n","\n","})\n","\n","main(cli_args)"]},{"cell_type":"markdown","metadata":{"id":"aDxUvYaleeBe"},"source":["# Inference"]},{"cell_type":"code","source":["import collections\n","\n","def get_score1(args):\n","  tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n","      \"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624\",\n","      do_lower_case=False,\n","  )\n","  dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n","  cof = [1, 1]\n","  best_cof = [1]\n","  all_scores = collections.OrderedDict()\n","  idx = 0\n","  for input_file in args.input_null_files.split(\",\"):\n","    with open(input_file, 'r') as reader:\n","      input_data = json.load(reader, strict=False)\n","      for (key, score) in input_data.items():\n","        if key not in all_scores:\n","          all_scores[key] = []\n","        all_scores[key].append(cof[idx] * score)\n","    idx += 1\n","  output_scores = {}\n","  for (key, scores) in all_scores.items():\n","    mean_score = 0.0\n","    for score in scores:\n","      mean_score += score\n","    mean_score /= float(len(scores))\n","    output_scores[key] = mean_score\n","\n","  all_nbest = collections.OrderedDict()\n","\n","  with open(args.input_nbest_files, 'r') as reader:\n","    input_data = json.load(reader, strict=False)\n","    for (key, entries) in input_data.items():\n","      if key not in all_nbest:\n","        all_nbest[key] = collections.defaultdict(float)\n","      for entry in entries:\n","        all_nbest[key][entry[\"text\"]] += best_cof[0] * entry['probability']\n","  output_predictions = {}\n","  for (key, entry_map) in all_nbest.items():\n","    # sorted_texts = sorted(\n","    #     entry_map.keys(), key=lambda x: entry[x], reverse=True)\n","    best_text = list(entry_map.keys())[0]\n","    output_predictions[key] = best_text\n","  \n","  best_th = args.thresh\n","\n","  for qid in output_predictions.keys():\n","    if output_scores[qid] > best_th:\n","      output_predictions[qid] = \"\"\n","  \n","\n","  output_prediction_file = \"/content/drive/MyDrive/Colab Notebooks/KB/final_predictions.json\"\n","  with open(output_prediction_file, \"w\") as writer:\n","    writer.write(json.dumps(output_predictions, indent=4, ensure_ascii=False) + \"\\n\")\n","\n","  for example in examples:\n","    example.qas_id = str(example.qas_id)\n","  results = squad_evaluate(examples, output_predictions)\n","\n","  output_eval_file = \"/content/drive/MyDrive/Colab Notebooks/KB/final_results.json\"\n","\n","  with open(output_eval_file, \"w\", encoding='utf-8') as f:\n","      json.dump(results, f, ensure_ascii=False)"],"metadata":{"id":"6FVVW0JE0nx5","executionInfo":{"status":"ok","timestamp":1668736678956,"user_tz":-540,"elapsed":247,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import easydict \n","args = easydict.EasyDict({\n","    'input_null_files': \"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-12000/cls_score.json,/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624/null_odds_11624.json\",\n","    'input_nbest_files': \"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624/nbest_predictions_11624.json\",\n","    'thresh': 0.2,\n","    \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/KB/data\",\n","    \"model_name_or_path\": \"/content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2\",\n","    \"max_seq_length\": 512,\n","    \"model_type\": \"albert\"\n","})\n","\n","get_score1(args)"],"metadata":{"id":"l1hRWw8c05eD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668736706348,"user_tz":-540,"elapsed":22643,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}},"outputId":"14ddd322-5339-4954-b55a-e281071e7de1"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading features from cached file /content/drive/MyDrive/Colab Notebooks/KB/data/cached_dev_kb-albert-char-base-v2_512\n"]}]},{"cell_type":"markdown","source":["# DEPLOY"],"metadata":{"id":"DoNIe6GJ80ON"}},{"cell_type":"markdown","source":["## Install"],"metadata":{"id":"DsJI-a0BoBxP"}},{"cell_type":"code","source":["! pip install ratsnlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"G-9tk8v4XEed","executionInfo":{"status":"ok","timestamp":1668714760105,"user_tz":-540,"elapsed":10789,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}},"outputId":"e2588843-6f3f-492f-a2c5-2b9994af13ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ratsnlp\n","  Downloading ratsnlp-1.0.52-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 978 kB/s \n","\u001b[?25hCollecting flask-cors>=3.0.10\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Collecting transformers==4.10.0\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 8.4 MB/s \n","\u001b[?25hCollecting flask-ngrok>=0.0.25\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Collecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[K     |████████████████████████████████| 582 kB 82.8 MB/s \n","\u001b[?25hCollecting Korpora>=0.2.0\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.9.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.21.6)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (6.0)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n","\u001b[K     |████████████████████████████████| 529 kB 86.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (21.3)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.1.1)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.64.1)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.12.1+cu113)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2022.10.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 75.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (4.13.0)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.11.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (3.8.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 58.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2.23.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n","Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (3.8.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (2.1.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n","Collecting dataclasses>=0.6\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Collecting xlrd>=1.2.0\n","  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n","\u001b[K     |████████████████████████████████| 96 kB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.1->ratsnlp) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2022.9.24)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.50.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.14.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.38.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.19.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.6.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0->ratsnlp) (3.10.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.2.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=c02a5c453674c85277dc417bf47da4f777adf78f15a86e3b535ba92948ac4f5a\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: xlrd, torchmetrics, tokenizers, sacremoses, pyDeprecate, dataclasses, transformers, pytorch-lightning, Korpora, flask-ngrok, flask-cors, ratsnlp\n","  Attempting uninstall: xlrd\n","    Found existing installation: xlrd 1.1.0\n","    Uninstalling xlrd-1.1.0:\n","      Successfully uninstalled xlrd-1.1.0\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.2\n","    Uninstalling tokenizers-0.13.2:\n","      Successfully uninstalled tokenizers-0.13.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.24.0\n","    Uninstalling transformers-4.24.0:\n","      Successfully uninstalled transformers-4.24.0\n","Successfully installed Korpora-0.2.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 ratsnlp-1.0.52 sacremoses-0.0.53 tokenizers-0.10.3 torchmetrics-0.10.3 transformers-4.10.0 xlrd-2.0.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses","tokenizers","transformers"]}}},"metadata":{}}]},{"cell_type":"code","source":["def _is_whitespace(c):\n","    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","        return True\n","    return False\n","    \n","class SquadExample:\n","    \"\"\"\n","    A single training/test example for the Squad dataset, as loaded from disk.\n","\n","    Args:\n","        qas_id: The example's unique identifier\n","        question_text: The question string\n","        context_text: The context string\n","        answer_text: The answer string\n","        start_position_character: The character position of the start of the answer\n","        title: The title of the example\n","        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.\n","        is_impossible: False by default, set to True if the example has no possible answer.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        qas_id,\n","        question_text,\n","        context_text,\n","        answer_text,\n","        start_position_character,\n","        title,\n","        answers=[],\n","        is_impossible=False,\n","    ):\n","        self.qas_id = qas_id\n","        self.question_text = question_text\n","        self.context_text = context_text\n","        self.answer_text = answer_text\n","        self.title = title\n","        self.is_impossible = is_impossible\n","        self.answers = answers\n","\n","        self.start_position, self.end_position = 0, 0\n","\n","        doc_tokens = []\n","        char_to_word_offset = []\n","        prev_is_whitespace = True\n","\n","        # Split on whitespace so that different tokens may be attributed to their original position.\n","        for c in self.context_text:\n","            if _is_whitespace(c):\n","                prev_is_whitespace = True\n","            else:\n","                if prev_is_whitespace:\n","                    doc_tokens.append(c)\n","                else:\n","                    doc_tokens[-1] += c\n","                prev_is_whitespace = False\n","            char_to_word_offset.append(len(doc_tokens) - 1)\n","\n","        self.doc_tokens = doc_tokens\n","        self.char_to_word_offset = char_to_word_offset\n","\n","        # Start and end positions only has a value during evaluation.\n","        if start_position_character is not None and not is_impossible:\n","            self.start_position = char_to_word_offset[start_position_character]\n","            self.end_position = char_to_word_offset[\n","                min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)\n","            ]"],"metadata":{"id":"lrt8HdV07-Jg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"MYIKNslioM-p"}},{"cell_type":"code","source":["import easydict\n","import collections\n","def inference(question, context):\n","  args = easydict.EasyDict({\n","    'input_null_files': \"/content/drive/MyDrive/Colab Notebooks/KB/inference/cls_score.json,/content/drive/MyDrive/Colab Notebooks/KB/inference/null_odds.json\",\n","    'input_nbest_files':  \"/content/drive/MyDrive/Colab Notebooks/KB/inference/nbest_predictions.json\",\n","    'thresh': 0,\n","    \"task\": \"news\",\n","    \"threads\": 20,\n","    \"version_2_with_negative\": True,\n","    \"null_score_diff_threshold\": 0.0,\n","    \"max_seq_length\": 512,\n","    \"doc_stride\": 400,\n","    \"max_query_length\": 100,\n","    \"max_answer_length\": 100,\n","    \"n_best_size\": 20,\n","    \"verbose_logging\": True,\n","    \"overwrite_output_dir\": True,\n","    \"evaluate_during_training\": True,\n","    \"eval_all_checkpoints\": True,\n","    \"save_optimizer\": False,\n","    \"do_lower_case\": False,\n","    \"n_gpu\": 1,\n","    \"adam_epsilon\": 1e-8,\n","    \"warmup_proportion\": 0,\n","    \"max_steps\": -1,\n","    \"max_grad_norm\": 1.0,\n","    \"no_cuda\": False,\n","    \"model_type\": \"albert\",\n","    \"model_name_or_path\": \"/content/drive/MyDrive/Colab Notebooks/KB/model/kb-albert-char-base-v2\",\n","    \"output_dir\": \"/content/drive/MyDrive/Colab Notebooks/KB/inference\",\n","    \"seed\": 42,\n","    \"train_batch_size\": 16,\n","    \"eval_batch_size\": 128,\n","    \"logging_steps\": 4000,\n","    \"save_steps\": 4000,\n","    \"learning_rate\": 5e-5,    \n","    })\n","  args.device = \"cpu\"\n","  processor = SquadV2Processor()\n","  tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624\"\n","                                                                 , do_lower_case=args.do_lower_case,\n","      )\n","\n","  examples = []\n","  title = \"title\"\n","  context_text = context\n","  qas_id = 0\n","  question_text = question\n","  start_position_character = None\n","  answer_text = None\n","  answers = [{\n","      \"text\": \"\",\n","      \"answer_start\": 0,\n","      \"clue_start\": 0,\n","      \"clue_text\": 0,\n","      \"options\": 0\n","      }]\n","  is_impossible = False\n","\n","  example = SquadExample(\n","      qas_id=qas_id,\n","      question_text=question_text,\n","      context_text=context_text,\n","      answer_text=answer_text,\n","      start_position_character=start_position_character,\n","      title=title,\n","      is_impossible=is_impossible,\n","      answers=answers,\n","  )\n","  examples.append(example)\n","\n","  features, dataset = squad_convert_examples_to_features(\n","      examples=examples,\n","      tokenizer=tokenizer,\n","      max_seq_length=args.max_seq_length,\n","      doc_stride=args.doc_stride,\n","      max_query_length=args.max_query_length,\n","      is_training=False,\n","      return_dataset=\"pt\",\n","      threads=args.threads,\n","  )\n","\n","  ## SKETCHY MODULE\n","  config = CONFIG_CLASSES[args.model_type].from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-12000\")\n","\n","  model = MODEL_FOR_SEQUENCE_CLASSIFICATION[args.model_type].from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/SketchReadingModule/checkpoint-12000\", config = config)\n","\n","  eval_sampler = SequentialSampler(dataset)\n","  eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n","\n","\n","  # Inference\n","  print(\"***** Running Sketchy *****\")\n","\n","  num_id = 0\n","  preds = None\n","  key_map = {}\n","  cnt_map = {}\n","\n","  id_map = [feature.qas_id for feature in features]\n","\n","\n","  for batch in progress_bar(eval_dataloader):\n","      model.eval()\n","      batch = tuple(t.to(args.device) for t in batch)\n","\n","      with torch.no_grad():\n","          inputs = {'input_ids': batch[0],\n","                  'attention_mask': batch[1],\n","                  'token_type_ids': batch[2]\n","                  }\n","\n","          logits = model(**inputs)\n","          logits = logits[0].detach().cpu().numpy()\n","\n","      # batch내의 모든 예측값에 대해서\n","      for logit in logits:\n","          qas_id = id_map[num_id]\n","          \n","          if qas_id in key_map:\n","              logit_list = key_map[qas_id]\n","              logit_list[0] += logit[0]\n","              logit_list[1] += logit[1]\n","              cnt_map[qas_id] += 1\n","\n","          else:\n","              cnt_map[qas_id] = 1\n","              key_map[qas_id] = [logit[0], logit[1]]\n","          \n","          num_id += 1\n","\n","\n","  final_map = {}\n","  for idx, key in enumerate(key_map):\n","      key_list = key_map[key]\n","      key_list[0] = key_list[0] / cnt_map[key]\n","      key_list[1] = key_list[1] / cnt_map[key]\n","      final_map[key] = key_list[1] - key_list[0]\n","\n","  with open(os.path.join(args.output_dir, \"cls_score.json\"), \"w\") as writer:\n","      writer.write(json.dumps(final_map, indent=4, ensure_ascii=False) + \"\\n\")\n","\n","  ## INTENSIVE MODULE\n","\n","  config = CONFIG_CLASSES[args.model_type].from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624\")\n","  model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/KB/ckpt/IntensiveReadingModule/checkpoint-11624\", config=config)\n","\n","\n","  # Eval!\n","  print(\"***** Running INTENSIVE *****\")\n","\n","  all_results = []\n","\n","  for batch in progress_bar(eval_dataloader):\n","      model.eval()\n","      batch = tuple(t.to(args.device) for t in batch)\n","\n","      with torch.no_grad():\n","          inputs = {\n","              \"input_ids\": batch[0],\n","              \"attention_mask\": batch[1],\n","              \"token_type_ids\": batch[2],\n","          }\n","\n","          if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n","              del inputs[\"token_type_ids\"]\n","\n","          example_indices = batch[3]\n","\n","          outputs = model(**inputs)\n","\n","      for i, example_index in enumerate(example_indices):\n","          eval_feature = features[example_index.item()]\n","          unique_id = int(eval_feature.unique_id)\n","\n","          output = [to_list(output[i]) for output in outputs.values()]\n","\n","          start_logits, end_logits = output\n","          result = SquadResult(unique_id, start_logits, end_logits)\n","\n","          all_results.append(result)\n","\n","  # Compute predictions\n","  output_prediction_file = os.path.join(args.output_dir, \"predictions.json\")\n","  output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions.json\")\n","\n","  output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds.json\")\n","\n","  predictions = compute_predictions_logits(\n","      examples,\n","      features,\n","      all_results,\n","      args.n_best_size,\n","      args.max_answer_length,\n","      args.do_lower_case,\n","      output_prediction_file,\n","      output_nbest_file,\n","      output_null_log_odds_file,\n","      args.verbose_logging,\n","      args.version_2_with_negative,\n","      args.null_score_diff_threshold,\n","      tokenizer,\n","  )\n","\n","\n","  # REAR VERIFICATION\n","\n","  cof = [1, 1]\n","  best_cof = [1]\n","  all_scores = collections.OrderedDict()\n","  idx = 0\n","  for input_file in args.input_null_files.split(\",\"):\n","    with open(input_file, 'r') as reader:\n","      input_data = json.load(reader, strict=False)\n","      for (key, score) in input_data.items():\n","        if key not in all_scores:\n","          all_scores[key] = []\n","        all_scores[key].append(cof[idx] * score)\n","    idx += 1\n","  output_scores = {}\n","  for (key, scores) in all_scores.items():\n","    mean_score = 0.0\n","    for score in scores:\n","      mean_score += score\n","    mean_score /= float(len(scores))\n","    output_scores[key] = mean_score\n","\n","  all_nbest = collections.OrderedDict()\n","\n","  with open(args.input_nbest_files, 'r') as reader:\n","    input_data = json.load(reader, strict=False)\n","    for (key, entries) in input_data.items():\n","      if key not in all_nbest:\n","        all_nbest[key] = collections.defaultdict(float)\n","      for entry in entries:\n","        all_nbest[key][entry[\"text\"]] += best_cof[0] * entry['probability']\n","  output_predictions = {}\n","  for (key, entry_map) in all_nbest.items():\n","    best_text = list(entry_map.keys())[0]\n","    output_predictions[key] = best_text\n","  \n","  best_th = args.thresh\n","\n","  for qid in output_predictions.keys():\n","    if output_scores[qid] > best_th:\n","      output_predictions[qid] = \"\"\n","  answer = \"\".join(output_predictions.values())\n","  return {\n","      'question': question,\n","      'context': context,\n","      'answer': answer\n","  }"],"metadata":{"id":"_sqUTOhAt7ox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## test"],"metadata":{"id":"FwyFchOxoRWz"}},{"cell_type":"code","source":["question = '서로의 채무와 채권을 같은 액수만큼 소멸시키는 것을 무엇이라 하나요'\n","context = \"상계란 채무자와 은행이 서로에 대해 금전 채무와 채권을 가지는 경우에, 일방적 의사표시로  서로의 채무와 채권을 같은 액수만큼 소멸시키는 것을 말합니다. 은행은 대출 등 채무의 변제기가 도래하였거나 채무자가 기한의 이익을 상실한 경우, 채무자의 대출 등 그 채무와 채무자의 은행에 대한 예금 기타의 채권을 그채권의 기한이 도래하지 않았어도 서면통지에 의하여 상계할 수 있습니다.\"\n","inference(question, context)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"GtT9J9fUbdvS","executionInfo":{"status":"ok","timestamp":1668714738628,"user_tz":-540,"elapsed":4740,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}},"outputId":"1a3427a2-82f8-49ef-ef83-b2ef84295e58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 210.72it/s]\n","add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["***** Running Sketchy *****\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["***** Running INTENSIVE *****\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'question': '서로의 채무와 채권을 같은 액수만큼 소멸시키는 것을 무엇이라 하나요',\n"," 'context': '상계란 채무자와 은행이 서로에 대해 금전 채무와 채권을 가지는 경우에, 일방적 의사표시로  서로의 채무와 채권을 같은 액수만큼 소멸시키는 것을 말합니다. 은행은 대출 등 채무의 변제기가 도래하였거나 채무자가 기한의 이익을 상실한 경우, 채무자의 대출 등 그 채무와 채무자의 은행에 대한 예금 기타의 채권을 그채권의 기한이 도래하지 않았어도 서면통지에 의하여 상계할 수 있습니다.',\n"," 'answer': '상계'}"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## WEB"],"metadata":{"id":"FoVDu_6aoTEr"}},{"cell_type":"code","source":["!mkdir /root/.ngrok2 && echo \"authtoken: 2HgLSGNA1ZMv6lFdhE3Wvt92Qts_QDnBDxTkFsMVe1DEigU\" > /root/.ngrok2/ngrok.yml"],"metadata":{"id":"Xs19x_XdZ794","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668714840870,"user_tz":-540,"elapsed":416,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}},"outputId":"409785b2-4b56-43e8-ac6b-c28bff9e8a10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/root/.ngrok2’: File exists\n"]}]},{"cell_type":"code","source":["from ratsnlp.nlpbook.qa import get_web_service_app\n","\n","app = get_web_service_app(inference)\n","app.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"id":"qqQj2TTDXU5e","executionInfo":{"status":"ok","timestamp":1668716466857,"user_tz":-540,"elapsed":201544,"user":{"displayName":"Lainie Kang","userId":"17932680864583243195"}},"outputId":"6aeca914-48db-432e-90f0-b45e616a3d60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"ratsnlp.nlpbook.qa.deploy\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://8157-34-173-25-172.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [17/Nov/2022 20:17:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [17/Nov/2022 20:17:50] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 99.52it/s]\n","add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]"]},{"output_type":"stream","name":"stdout","text":["***** Running Sketchy *****\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["***** Running INTENSIVE *****\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [17/Nov/2022 20:18:56] \"\u001b[37mPOST /api HTTP/1.1\u001b[0m\" 200 -\n","convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 94.63it/s]\n","add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 13315.25it/s]"]},{"output_type":"stream","name":"stdout","text":["***** Running Sketchy *****\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["***** Running INTENSIVE *****\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    /* Turns off some styling */\n","    progress {\n","        /* gets rid of default border in Firefox and Opera. */\n","        border: none;\n","        /* Needs to be in here for Safari polyfill so background images work as expected. */\n","        background-size: auto;\n","    }\n","    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n","        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n","    }\n","    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","        background: #F44336;\n","    }\n","</style>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [1/1 00:00&lt;00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [17/Nov/2022 20:20:11] \"\u001b[37mPOST /api HTTP/1.1\u001b[0m\" 200 -\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Ty4IOYDUxJOA","ahOvK-POjc30","_Ev7GgGhWPYb","aDxUvYaleeBe"],"provenance":[],"mount_file_id":"10zwNQ8Lcn2FNkG0ONAG1dmOhQrAQ6kCZ","authorship_tag":"ABX9TyPI5yn3rQPGr/1hOpqrKJuI"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}